{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install required packages\n!pip install -q transformers datasets evaluate rouge_score accelerate sacrebleu nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-23T05:46:53.136356Z","iopub.execute_input":"2025-07-23T05:46:53.136672Z","iopub.status.idle":"2025-07-23T05:47:04.210317Z","shell.execute_reply.started":"2025-07-23T05:46:53.136639Z","shell.execute_reply":"2025-07-23T05:47:04.209434Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Step 2: Imports and setup\nimport os\nimport torch\nimport numpy as np\nimport nltk\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nfrom transformers import (\n    AutoTokenizer, AutoModelForSeq2SeqLM, \n    Seq2SeqTrainingArguments, Seq2SeqTrainer, \n    DataCollatorForSeq2Seq, pipeline\n)\nfrom datasets import load_dataset\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T05:47:04.216093Z","iopub.execute_input":"2025-07-23T05:47:04.216425Z","iopub.status.idle":"2025-07-23T05:47:32.049302Z","shell.execute_reply.started":"2025-07-23T05:47:04.216400Z","shell.execute_reply":"2025-07-23T05:47:32.048696Z"}},"outputs":[{"name":"stderr","text":"2025-07-23 05:47:17.502865: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753249637.727542      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753249637.791928      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Step 3: Login to Hugging Face\nprint(\"=== Setting up Hugging Face authentication ===\")\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T05:47:32.050004Z","iopub.execute_input":"2025-07-23T05:47:32.050485Z","iopub.status.idle":"2025-07-23T05:47:32.289240Z","shell.execute_reply.started":"2025-07-23T05:47:32.050465Z","shell.execute_reply":"2025-07-23T05:47:32.288564Z"}},"outputs":[{"name":"stdout","text":"=== Setting up Hugging Face authentication ===\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Step 4: Check GPU and system info\nprint(\"=== System Information ===\")\nif torch.cuda.is_available():\n    print(\"GPU is available!\")\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n    print(\"GPU not available. Training will run on CPU.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:32:09.327915Z","iopub.execute_input":"2025-07-21T17:32:09.328120Z","iopub.status.idle":"2025-07-21T17:32:09.785064Z","shell.execute_reply.started":"2025-07-21T17:32:09.328102Z","shell.execute_reply":"2025-07-21T17:32:09.784450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Load model and tokenizer\nprint(\"\\n=== Loading Model and Tokenizer ===\")\nmodel_name = \"facebook/bart-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\nmodel.config.no_repeat_ngram_size = 3  # Prevent repeating 3-word sequences\nmodel.config.length_penalty = 1.0      # No penalty on length (default is usually 1.0)\n\nprint(f\"Model loaded: {model_name}\")\nprint(f\"Model parameters: {model.num_parameters():,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T05:47:32.291171Z","iopub.execute_input":"2025-07-23T05:47:32.291403Z","iopub.status.idle":"2025-07-23T05:47:36.238072Z","shell.execute_reply.started":"2025-07-23T05:47:32.291386Z","shell.execute_reply":"2025-07-23T05:47:36.237458Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Model and Tokenizer ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"361e8b0bb5864d399745b445ccb162b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cdd1c4356fc44a09502cdafacad34cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55ce7d1dc9ea4521962046c07038009c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c083226c7af04e9092d68d9b3822c074"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3761ae8b00c423098452f0540ae5eb4"}},"metadata":{}},{"name":"stdout","text":"Model loaded: facebook/bart-base\nModel parameters: 139,420,416\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Step 6: Load and prepare dataset\nprint(\"\\n=== Loading Dataset ===\")\ndataset = load_dataset(\"xsum\", trust_remote_code=True)\n\n# Use smaller subsets for faster training and iteration\ntrain_size = 100000\nval_size = 10000\n\ntrain_dataset = dataset[\"train\"].select(range(train_size))\nval_dataset = dataset[\"validation\"].select(range(val_size))\n\nprint(f\"Original dataset size: {len(dataset['train']):,} training examples\")\nprint(f\"Using subset: {train_size:,} training examples\")\nprint(f\"Validation subset: {val_size:,} examples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Show sample data\nprint(\"\\n=== Sample Data ===\")\nsample = train_dataset[0]\nprint(f\"Article preview: {sample['document'][:300]}...\")\nprint(f\"Summary: {sample['summary']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:33:25.054184Z","iopub.execute_input":"2025-07-21T17:33:25.054585Z","iopub.status.idle":"2025-07-21T17:33:25.059277Z","shell.execute_reply.started":"2025-07-21T17:33:25.054556Z","shell.execute_reply":"2025-07-21T17:33:25.058673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 8: Preprocessing function\nprint(\"\\n=== Setting up Data Preprocessing ===\")\nprefix = \"summarize: \"\nmax_input_length = 512\nmax_target_length = 64\n\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"document\"]]\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n    \n    # Setup the tokenizer for targets\n    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_target_length, truncation=True)\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing\nprint(\"Preprocessing training data...\")\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\n\nprint(\"Preprocessing validation data...\")\ntokenized_val = val_dataset.map(preprocess_function, batched=True)\n\nprint(f\"Tokenized training examples: {len(tokenized_train):,}\")\nprint(f\"Tokenized validation examples: {len(tokenized_val):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T05:47:36.238753Z","iopub.execute_input":"2025-07-23T05:47:36.239007Z","iopub.status.idle":"2025-07-23T05:47:36.512803Z","shell.execute_reply.started":"2025-07-23T05:47:36.238974Z","shell.execute_reply":"2025-07-23T05:47:36.511865Z"}},"outputs":[{"name":"stdout","text":"\n=== Setting up Data Preprocessing ===\nPreprocessing training data...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4177734259.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Apply preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocessing training data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtokenized_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preprocessing validation data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"],"ename":"NameError","evalue":"name 'train_dataset' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"# Step 9: Setup evaluation metrics\nprint(\"\\n=== Setting up Evaluation Metrics ===\")\n\n# Download NLTK data required for ROUGE\nnltk.download('punkt', quiet=True)\n\n# Load ROUGE metric\nrouge_metric = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n\n    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n    \n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # ROUGE expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n    \n    # Compute ROUGE scores\n    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    \n    # Extract key results and round\n    result = {key: round(value * 100, 2) for key, value in result.items()}\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:19:07.780544Z","iopub.execute_input":"2025-07-21T19:19:07.781136Z","iopub.status.idle":"2025-07-21T19:19:08.372008Z","shell.execute_reply.started":"2025-07-21T19:19:07.781113Z","shell.execute_reply":"2025-07-21T19:19:08.371475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10: Training arguments\nprint(\"\\n=== Setting up Training Arguments ===\")\n\nYOUR_HF_USERNAME = \"souradeepdutta\"\nMODEL_HUB_ID = f\"{YOUR_HF_USERNAME}/bart-base-summarizer-xsum\"\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./bart-base-summarizer-xsum\",\n    \n    # Evaluation strategy\n    eval_strategy=\"steps\",\n    eval_steps=3000, \n    save_strategy=\"steps\",\n    save_steps=3000,\n    \n    # Batch sizes\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32, \n    gradient_accumulation_steps=2,\n    \n    # Learning parameters\n    learning_rate=5e-5, #0.00005 \n    weight_decay=0.01,\n    warmup_steps=500,\n    \n    # Training duration\n    num_train_epochs=3,\n    max_steps=-1,\n    \n    # Performance optimizations for GPU\n    fp16=True,\n\n    report_to=\"none\",\n    \n    # Logging\n    logging_steps=100,\n    logging_strategy=\"steps\",\n    \n    # Model saving and pushing to hub\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"rouge1\",\n    push_to_hub=True,\n    hub_model_id=MODEL_HUB_ID,\n    \n    # Generation settings for evaluation\n    predict_with_generate=True,\n    generation_max_length=64,\n    generation_num_beams=4,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:34:49.884487Z","iopub.execute_input":"2025-07-21T17:34:49.884746Z","iopub.status.idle":"2025-07-21T17:34:49.921122Z","shell.execute_reply.started":"2025-07-21T17:34:49.884723Z","shell.execute_reply":"2025-07-21T17:34:49.920352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 11: Data collator\nprint(\"\\n=== Setting up Data Collator ===\")\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model,\n    padding=True,\n    pad_to_multiple_of=8,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:34:49.923414Z","iopub.execute_input":"2025-07-21T17:34:49.923672Z","iopub.status.idle":"2025-07-21T17:34:49.928709Z","shell.execute_reply.started":"2025-07-21T17:34:49.923654Z","shell.execute_reply":"2025-07-21T17:34:49.927996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Creating a smaller validation set for faster intermediate evaluations...\")\nsmall_eval_dataset = tokenized_val.select(range(1000))\nprint(f\"Using {len(small_eval_dataset)} examples for intermediate evaluation.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:34:49.929641Z","iopub.execute_input":"2025-07-21T17:34:49.929863Z","iopub.status.idle":"2025-07-21T17:34:49.947496Z","shell.execute_reply.started":"2025-07-21T17:34:49.929848Z","shell.execute_reply":"2025-07-21T17:34:49.946818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 12: Create trainer\nprint(\"\\n=== Creating Trainer ===\")\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=small_eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:34:49.948257Z","iopub.execute_input":"2025-07-21T17:34:49.948780Z","iopub.status.idle":"2025-07-21T17:34:50.445197Z","shell.execute_reply.started":"2025-07-21T17:34:49.948756Z","shell.execute_reply":"2025-07-21T17:34:50.444464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 13: Model optimizations for memory\nprint(\"\\n=== Applying Model Optimizations ===\")\n\n# Gradient checkpointing saves memory at the cost of a slightly slower backward pass.\n#model.gradient_checkpointing_enable()\n\n# Disable cache during training, as it's only used for inference.\nif hasattr(model.config, 'use_cache'):\n    model.config.use_cache = False","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-23T05:45:43.759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 14: Start training\nprint(\"\\n=== Starting Training ===\")\n\ntry:\n    trainer.train(resume_from_checkpoint=False)\n    print(\"\\n✅ Training completed successfully!\")\nexcept Exception as e:\n    print(f\"\\n❌ Training failed: {str(e)}\")\n    raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T19:22:16.931934Z","iopub.execute_input":"2025-07-21T19:22:16.932291Z","iopub.status.idle":"2025-07-21T20:55:29.152861Z","shell.execute_reply.started":"2025-07-21T19:22:16.932262Z","shell.execute_reply":"2025-07-21T20:55:29.151233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 15: Save and push model to Hub\nprint(\"\\n=== Saving and Pushing Final Model ===\")\ntry:\n    trainer.save_model()\n    trainer.push_to_hub()\n    print(f\"✅ Model saved and pushed to {MODEL_HUB_ID} on the Hugging Face Hub!\")\nexcept Exception as e:\n    print(f\"❌ Error saving or pushing to hub: {str(e)}\")\n    print(\"Model was saved locally in ./bart-base-summarizer-xsum/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T20:55:29.153376Z","iopub.status.idle":"2025-07-21T20:55:29.153645Z","shell.execute_reply.started":"2025-07-21T20:55:29.153535Z","shell.execute_reply":"2025-07-21T20:55:29.153546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 16: Test the fine-tuned model\nprint(\"\\n=== Testing the Trained Model ===\")\ntry:\n    # Load the fine-tuned model from the local directory for inference\n    summarizer = pipeline(\n        \"summarization\", \n        model=\"souradeepdutta/bart-base-summarizer-xsum\",\n        tokenizer=tokenizer\n    )\n    \n    test_article = \"\"\"\n    NASA's James Webb Space Telescope has captured its first direct image of a planet outside our solar system. \n    The exoplanet, known as HIP 65426 b, is a gas giant about six to 12 times the mass of Jupiter. \n    This observation is a transformative moment for astronomy, as it points the way toward future observations \n    that will reveal more information than ever before about exoplanets. The telescope's advanced infrared \n    capabilities allow it to see past the glare of the host star to capture the faint planet.\n    \"\"\"\n    \n    print(f\"--- Test Article ---\")\n    print(f\"Original: {test_article.strip()}\")\n    \n    summary = summarizer(\n        test_article, \n        max_length=80, \n        min_length=15, \n        do_sample=False\n    )\n    \n    print(f\"\\nGenerated Summary: {summary[0]['summary_text']}\")\n        \nexcept Exception as e:\n    print(f\"❌ Error during testing: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T20:55:29.154589Z","iopub.status.idle":"2025-07-21T20:55:29.154862Z","shell.execute_reply.started":"2025-07-21T20:55:29.154743Z","shell.execute_reply":"2025-07-21T20:55:29.154755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 17: Final model evaluation on the test set\nprint(\"\\n=== Final Model Evaluation on Test Set ===\")\ntry:\n    # Evaluate on a small subset of the test set\n    test_dataset = dataset[\"test\"].select(range(5000))\n    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n    \n    print(\"Running evaluation...\")\n    eval_results = trainer.evaluate(tokenized_test)\n    \n    print(\"\\n--- Test Set ROUGE Scores ---\")\n    for key, value in eval_results.items():\n        if 'rouge' in key:\n            print(f\"  {key}: {value}\")\n        \nexcept Exception as e:\n    print(f\"❌ Error during final evaluation: {str(e)}\")\n\nprint(\"\\n=== Notebook Complete! ===\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T20:55:29.155691Z","iopub.status.idle":"2025-07-21T20:55:29.155981Z","shell.execute_reply.started":"2025-07-21T20:55:29.155816Z","shell.execute_reply":"2025-07-21T20:55:29.155832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 18: Interactive Summarization DemoPaste your article herePaste your article here......\nimport ipywidgets as widgets\nfrom IPython.display import display\n\nprint(\"\\n=== Interactive Summarization Demo ===\")\n\ntry:\n    # 1. Load the fine-tuned model and tokenizer\n    print(\"Loading your fine-tuned model...\")\n    model_path = \"souradeepdutta/bart-base-summarizer-xsum\"\n    summarizer = pipeline(\n        \"summarization\", \n        model=model_path,\n        tokenizer=model_path,\n        device=0\n    )\n    print(\"✅ Model loaded successfully!\")\n\n    # 2. Create a text area for user input\n    print(\"\\nPaste your article into the text box below and click 'Summarize'.\")\n    article_input = widgets.Textarea(\n        value='Paste your article herePaste your article here......',\n        placeholder='Type something',\n        description='Article:',\n        layout={'height': '200px', 'width': '95%'},\n        disabled=False\n    )\n\n    # 3. Create a button to trigger summarization\n    summarize_button = widgets.Button(\n        description='Summarize',\n        button_style='success',\n        tooltip='Click to generate summary',\n        icon='check'\n    )\n\n    # 4. Create an output area to display the result\n    summary_output = widgets.Output()\n\n    # 5. Define the function to run on button click\n    def on_summarize_button_clicked(b):\n        with summary_output:\n            summary_output.clear_output() # Clear previous summary\n            print(\"Generating summary...\")\n            \n            # Get the text and generate the summary\n            article_text = article_input.value\n            if not article_text or article_text == 'Paste your article here...':\n                print(\"❌ Please paste an article first.\")\n                return\n\n            try:\n                # Generate summary with sensible length constraints\n                result = summarizer(\n                    article_text, \n                    max_length=128,\n                    min_length=30,\n                    do_sample=False,\n                    num_beams=4,\n                    # temperature=0.8,\n                    # top_p=0.95,\n                    # top_k = 50,\n                    # no_repeat_ngram_size=3\n                )\n                \n                print(\"\\n--- Generated Summary ---\")\n                print(result[0]['summary_text'])\n                \n            except Exception as e:\n                print(f\"An error occurred during summarization: {e}\")\n\n    # 6. Link the button to the function\n    summarize_button.on_click(on_summarize_button_clicked)\n\n    # 7. Display the widgets\n    display(article_input, summarize_button, summary_output)\n\nexcept Exception as e:\n    print(f\"\\n❌ An error occurred while setting up the demo: {str(e)}\")\n    print(\"Please ensure that you have successfully trained and saved the model in the './bart-base-summarizer-xsum' directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T06:45:19.629164Z","iopub.execute_input":"2025-07-23T06:45:19.629900Z","iopub.status.idle":"2025-07-23T06:45:20.305485Z","shell.execute_reply.started":"2025-07-23T06:45:19.629871Z","shell.execute_reply":"2025-07-23T06:45:20.304698Z"}},"outputs":[{"name":"stdout","text":"\n=== Interactive Summarization Demo ===\nLoading your fine-tuned model...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"✅ Model loaded successfully!\n\nPaste your article into the text box below and click 'Summarize'.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Textarea(value='Paste your article herePaste your article here......', description='Article:', layout=Layout(h…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e640574415364aea9bfa3af8367d3d90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Button(button_style='success', description='Summarize', icon='check', style=ButtonStyle(), tooltip='Click to g…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ef5edfe6b944b10b629f679c058e635"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a6d7d0462c54392af09a5617f89f764"}},"metadata":{}}],"execution_count":19}]}