{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install required packages\n!pip install -q transformers datasets evaluate rouge_score accelerate sacrebleu nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T04:52:52.333876Z","iopub.execute_input":"2025-07-24T04:52:52.334413Z","iopub.status.idle":"2025-07-24T04:54:21.780408Z","shell.execute_reply.started":"2025-07-24T04:52:52.334389Z","shell.execute_reply":"2025-07-24T04:54:21.779697Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0m00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Step 2: Imports and setup\nimport os\nimport torch\nimport numpy as np\nimport nltk\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nfrom transformers import (\n    AutoTokenizer, AutoModelForSeq2SeqLM, \n    Seq2SeqTrainingArguments, Seq2SeqTrainer, \n    DataCollatorForSeq2Seq, pipeline\n)\nfrom datasets import load_dataset\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T04:54:41.796581Z","iopub.execute_input":"2025-07-24T04:54:41.796904Z","iopub.status.idle":"2025-07-24T04:55:15.247524Z","shell.execute_reply.started":"2025-07-24T04:54:41.796880Z","shell.execute_reply":"2025-07-24T04:55:15.246690Z"}},"outputs":[{"name":"stderr","text":"2025-07-24 04:54:56.990118: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753332897.328914      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753332897.422733      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Step 3: Login to Hugging Face\nprint(\"=== Setting up Hugging Face authentication ===\")\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T04:55:15.658282Z","iopub.execute_input":"2025-07-24T04:55:15.658491Z","iopub.status.idle":"2025-07-24T04:55:15.858102Z","shell.execute_reply.started":"2025-07-24T04:55:15.658474Z","shell.execute_reply":"2025-07-24T04:55:15.857449Z"}},"outputs":[{"name":"stdout","text":"=== Setting up Hugging Face authentication ===\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Step 4: Check GPU and system info\nprint(\"=== System Information ===\")\nif torch.cuda.is_available():\n    print(\"GPU is available!\")\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n    print(\"GPU not available. Training will run on CPU.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T07:02:44.667904Z","iopub.execute_input":"2025-07-12T07:02:44.668173Z","iopub.status.idle":"2025-07-12T07:02:44.672682Z","shell.execute_reply.started":"2025-07-12T07:02:44.668140Z","shell.execute_reply":"2025-07-12T07:02:44.671929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Load model and tokenizer\nprint(\"\\n=== Loading Model and Tokenizer ===\")\nmodel_name = \"facebook/bart-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\nmodel.config.no_repeat_ngram_size = 3  # Prevent repeating 3-word sequences\nmodel.config.length_penalty = 1.0      # No penalty on length (default is usually 1.0)\n\nprint(f\"Model loaded: {model_name}\")\nprint(f\"Model parameters: {model.num_parameters():,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T04:55:30.262872Z","iopub.execute_input":"2025-07-24T04:55:30.263163Z","iopub.status.idle":"2025-07-24T04:55:38.076382Z","shell.execute_reply.started":"2025-07-24T04:55:30.263141Z","shell.execute_reply":"2025-07-24T04:55:38.075699Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Model and Tokenizer ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b6c93af09cc403fb3fe545b3589698f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ffb0f9d7fc040c0a48eea830838f09f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59cf9077d49143f4acbdd8a96fa33dff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12a70e8ca0454778a8dc696eeb381f6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f987d9660a5c4d23b061dfe5007caf00"}},"metadata":{}},{"name":"stdout","text":"Model loaded: facebook/bart-base\nModel parameters: 139,420,416\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Step 6: Load and prepare dataset\nprint(\"\\n=== Loading Dataset ===\")\ndataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n\ntrain_size = 100000 \nval_size = 10000   \n\ntrain_dataset = dataset[\"train\"].select(range(train_size))\nval_dataset = dataset[\"validation\"].select(range(val_size))\n\nprint(f\"Original dataset size: {len(dataset['train']):,} training examples\")\nprint(f\"Using subset: {train_size:,} training examples\")\nprint(f\"Validation subset: {val_size:,} examples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T07:02:51.431864Z","iopub.execute_input":"2025-07-12T07:02:51.432114Z","iopub.status.idle":"2025-07-12T07:03:06.875536Z","shell.execute_reply.started":"2025-07-12T07:02:51.432088Z","shell.execute_reply":"2025-07-12T07:03:06.874809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 7: Show sample data\nprint(\"\\n=== Sample Data ===\")\nsample = train_dataset[0]\nprint(f\"Article preview: {sample['article'][:300]}...\")\nprint(f\"Summary: {sample['highlights']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T07:03:06.877873Z","iopub.execute_input":"2025-07-12T07:03:06.878132Z","iopub.status.idle":"2025-07-12T07:03:06.882985Z","shell.execute_reply.started":"2025-07-12T07:03:06.878113Z","shell.execute_reply":"2025-07-12T07:03:06.882347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 8: Preprocessing function\nprint(\"\\n=== Setting up Data Preprocessing ===\")\nprefix = \"summarize: \"\nmax_input_length = 512  # Reduced for faster processing\nmax_target_length = 64 \n\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"article\"]]\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n    \n    # Setup the tokenizer for targets\n    labels = tokenizer(text_target=examples[\"highlights\"], max_length=max_target_length, truncation=True)\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing\nprint(\"Preprocessing training data...\")\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\n\nprint(\"Preprocessing validation data...\")\ntokenized_val = val_dataset.map(preprocess_function, batched=True)\n\nprint(f\"Tokenized training examples: {len(tokenized_train):,}\")\nprint(f\"Tokenized validation examples: {len(tokenized_val):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T07:03:06.883741Z","iopub.execute_input":"2025-07-12T07:03:06.883997Z","iopub.status.idle":"2025-07-12T07:03:29.121835Z","shell.execute_reply.started":"2025-07-12T07:03:06.883975Z","shell.execute_reply":"2025-07-12T07:03:29.121063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Setup evaluation metrics\nprint(\"\\n=== Setting up Evaluation Metrics ===\")\n\n# Download NLTK data required for ROUGE\nnltk.download('punkt', quiet=True)\n\n# Load ROUGE metric\nrouge_metric = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n\n    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n    \n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # ROUGE expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n    \n    # Compute ROUGE scores\n    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    \n    # Extract key results and round\n    result = {key: round(value * 100, 2) for key, value in result.items()}\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T07:03:29.122792Z","iopub.execute_input":"2025-07-12T07:03:29.123093Z","iopub.status.idle":"2025-07-12T07:03:30.002825Z","shell.execute_reply.started":"2025-07-12T07:03:29.123054Z","shell.execute_reply":"2025-07-12T07:03:30.002337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10: Training arguments\nprint(\"\\n=== Setting up Training Arguments ===\")\n\n# Make sure to change this to your Hugging Face username\nYOUR_HF_USERNAME = \"souradeepdutta\"\nMODEL_HUB_ID = f\"{YOUR_HF_USERNAME}/bart-base-summarizer\"\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"souradeepdutta/bart-base-summarizer\",\n    \n    # Evaluation strategy\n    eval_strategy=\"steps\",\n    eval_steps=3000, \n    save_strategy=\"steps\",\n    save_steps=3000,\n    \n    # Batch sizes\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32, \n    gradient_accumulation_steps=2,\n    \n    # Learning parameters\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_steps=500,\n    \n    # Training duration\n    num_train_epochs=3,\n    max_steps=-1,\n    \n    # Performance optimizations for GPU\n    fp16=True,\n\n    report_to=\"none\",\n    \n    # Logging\n    logging_steps=100,\n    logging_strategy=\"steps\",\n    \n    # Model saving and pushing to hub\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"rouge1\",\n    push_to_hub=True,\n    hub_model_id=MODEL_HUB_ID,\n    \n    # Generation settings for evaluation\n    predict_with_generate=True,\n    generation_max_length=64,\n    generation_num_beams=4,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T07:13:48.780656Z","iopub.execute_input":"2025-07-12T07:13:48.781496Z","iopub.status.idle":"2025-07-12T07:13:48.816908Z","shell.execute_reply.started":"2025-07-12T07:13:48.781461Z","shell.execute_reply":"2025-07-12T07:13:48.816181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 11: Data collator\nprint(\"\\n=== Setting up Data Collator ===\")\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model,\n    padding=True,\n    pad_to_multiple_of=8,  # Optimize for tensor cores on modern GPUs\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T07:13:48.818112Z","iopub.execute_input":"2025-07-12T07:13:48.818389Z","iopub.status.idle":"2025-07-12T07:13:48.822714Z","shell.execute_reply.started":"2025-07-12T07:13:48.818365Z","shell.execute_reply":"2025-07-12T07:13:48.821956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 12: Create trainer\nprint(\"\\n=== Creating Trainer ===\")\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T07:13:48.823544Z","iopub.execute_input":"2025-07-12T07:13:48.824295Z","iopub.status.idle":"2025-07-12T07:13:48.970094Z","shell.execute_reply.started":"2025-07-12T07:13:48.824274Z","shell.execute_reply":"2025-07-12T07:13:48.969442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 13: Model optimizations for memory\nprint(\"\\n=== Applying Model Optimizations ===\")\n\n# Gradient checkpointing saves memory at the cost of a slightly slower backward pass.\n#model.gradient_checkpointing_enable()\n\n# Disable cache during training, as it's only used for inference.\nif hasattr(model.config, 'use_cache'):\n    model.config.use_cache = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T07:13:48.971381Z","iopub.execute_input":"2025-07-12T07:13:48.971566Z","iopub.status.idle":"2025-07-12T07:13:48.976266Z","shell.execute_reply.started":"2025-07-12T07:13:48.971551Z","shell.execute_reply":"2025-07-12T07:13:48.975727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 14: Start training\nprint(\"\\n=== Starting Training ===\")\n\ntry:\n    trainer.train()\n    print(\"\\n✅ Training completed successfully!\")\nexcept Exception as e:\n    print(f\"\\n❌ Training failed: {str(e)}\")\n    raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T07:13:48.976902Z","iopub.execute_input":"2025-07-12T07:13:48.977220Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 15: Save and push model to Hub\nprint(\"\\n=== Saving and Pushing Final Model ===\")\ntry:\n    trainer.save_model()\n    trainer.push_to_hub()\n    print(f\"✅ Model saved and pushed to {MODEL_HUB_ID} on the Hugging Face Hub!\")\nexcept Exception as e:\n    print(f\"❌ Error saving or pushing to hub: {str(e)}\")\n    print(\"Model was saved locally in souradeepdutta/bart-base-summarizer/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 16: Test the fine-tuned model\nprint(\"\\n=== Testing the Trained Model ===\")\ntry:\n    # Load the fine-tuned model from the local directory for inference\n    summarizer = pipeline(\n        \"summarization\", \n        model=\"souradeepdutta/bart-base-summarizer\",\n        tokenizer=tokenizer\n    )\n    \n    test_article = \"\"\"\n    NASA's James Webb Space Telescope has captured its first direct image of a planet outside our solar system. \n    The exoplanet, known as HIP 65426 b, is a gas giant about six to 12 times the mass of Jupiter. \n    This observation is a transformative moment for astronomy, as it points the way toward future observations \n    that will reveal more information than ever before about exoplanets. The telescope's advanced infrared \n    capabilities allow it to see past the glare of the host star to capture the faint planet.\n    \"\"\"\n    \n    print(f\"--- Test Article ---\")\n    print(f\"Original: {test_article.strip()}\")\n    \n    summary = summarizer(\n        test_article, \n        max_length=80, \n        min_length=15, \n        do_sample=False\n    )\n    \n    print(f\"\\nGenerated Summary: {summary[0]['summary_text']}\")\n        \nexcept Exception as e:\n    print(f\"❌ Error during testing: {str(e)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 17: Final model evaluation on the test set\nprint(\"\\n=== Final Model Evaluation on Test Set ===\")\ntry:\n    # Evaluate on a small subset of the test set\n    test_dataset = dataset[\"test\"].select(range(5000))\n    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n    \n    print(\"Running evaluation...\")\n    eval_results = trainer.evaluate(tokenized_test)\n    \n    print(\"\\n--- Test Set ROUGE Scores ---\")\n    for key, value in eval_results.items():\n        if 'rouge' in key:\n            print(f\"  {key}: {value}\")\n        \nexcept Exception as e:\n    print(f\"❌ Error during final evaluation: {str(e)}\")\n\nprint(\"\\n=== Notebook Complete! ===\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 18: Interactive Summarization DemoPaste your article herePaste your article here......\nimport ipywidgets as widgets\nfrom IPython.display import display\n\nprint(\"\\n=== Interactive Summarization Demo ===\")\n\ntry:\n    # 1. Load the fine-tuned model and tokenizer\n    print(\"Loading your fine-tuned model...\")\n    model_path = \"souradeepdutta/bart-base-summarizer\"\n    summarizer = pipeline(\n        \"summarization\", \n        model=model_path,\n        tokenizer=model_path,\n        device=0\n    )\n    print(\"✅ Model loaded successfully!\")\n\n    # 2. Create a text area for user input\n    print(\"\\nPaste your article into the text box below and click 'Summarize'.\")\n    article_input = widgets.Textarea(\n        value='Paste your article herePaste your article here......',\n        placeholder='Type something',\n        description='Article:',\n        layout={'height': '200px', 'width': '95%'},\n        disabled=False\n    )\n\n    # 3. Create a button to trigger summarization\n    summarize_button = widgets.Button(\n        description='Summarize',\n        button_style='success',\n        tooltip='Click to generate summary',\n        icon='check'\n    )\n\n    # 4. Create an output area to display the result\n    summary_output = widgets.Output()\n\n    # 5. Define the function to run on button click\n    def on_summarize_button_clicked(b):\n        with summary_output:\n            summary_output.clear_output() # Clear previous summary\n            print(\"Generating summary...\")\n            \n            # Get the text and generate the summary\n            article_text = article_input.value\n            if not article_text or article_text == 'Paste your article here...':\n                print(\"❌ Please paste an article first.\")\n                return\n\n            try:\n                # Generate summary with sensible length constraints\n                result = summarizer(\n                    article_text, \n                    max_length=128,\n                    min_length=30,\n                    do_sample=True,\n                    num_beams=4,\n                    temperature=0.8,\n                    top_p=0.95,\n                    top_k = 50,\n                    no_repeat_ngram_size=3\n                )\n                \n                print(\"\\n--- Generated Summary ---\")\n                print(result[0]['summary_text'])\n                \n            except Exception as e:\n                print(f\"An error occurred during summarization: {e}\")\n\n    # 6. Link the button to the function\n    summarize_button.on_click(on_summarize_button_clicked)\n\n    # 7. Display the widgets\n    display(article_input, summarize_button, summary_output)\n\nexcept Exception as e:\n    print(f\"\\n❌ An error occurred while setting up the demo: {str(e)}\")\n    print(\"Please ensure that you have successfully trained and saved the model in the 'souradeepdutta/bart-base-summarizer' directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T05:00:38.019582Z","iopub.execute_input":"2025-07-24T05:00:38.019874Z","iopub.status.idle":"2025-07-24T05:00:38.901587Z","shell.execute_reply.started":"2025-07-24T05:00:38.019853Z","shell.execute_reply":"2025-07-24T05:00:38.900696Z"}},"outputs":[{"name":"stdout","text":"\n=== Interactive Summarization Demo ===\nLoading your fine-tuned model...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"✅ Model loaded successfully!\n\nPaste your article into the text box below and click 'Summarize'.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Textarea(value='Paste your article herePaste your article here......', description='Article:', layout=Layout(h…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9981e71e690465faf54df77cbfd99e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Button(button_style='success', description='Summarize', icon='check', style=ButtonStyle(), tooltip='Click to g…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"083e02a0a7d347b591871ac3a11de21a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b76d4357765d4ae3aeb3a4ae499c249a"}},"metadata":{}}],"execution_count":10}]}